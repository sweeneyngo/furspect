{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d21a1ca-81ee-4ecc-b70c-860080df4376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants initialized.\n"
     ]
    }
   ],
   "source": [
    "DATADIR = \"large_data/\"\n",
    "CATEGORIES = ['furry', 'notfurry']\n",
    "SIZE = 200\n",
    "RATIO = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "print('Constants initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6c1e2b-2ab4-4237-ae24-09ecfe930d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted.\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "# Create a ZipFile Object and load dataset.zip\n",
    "with ZipFile('large_dataset.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in dataset directory\n",
    "   zipObj.extractall('large_dataset')\n",
    "    \n",
    "print('Dataset extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11acb79-5a5a-4824-ba12-59e260b3695a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories are successfully created! Ready for modelling.\n"
     ]
    }
   ],
   "source": [
    "from os import makedirs, listdir, path     # create directories\n",
    "from shutil import copyfile                # move/swap files\n",
    "from random import seed, random            # generate random dispersion\n",
    "\n",
    "def create_dir(dataset_path):\n",
    "  # organize dataset into a useful structure\n",
    "\n",
    "  # create directories\n",
    "  subdirs = ['train', 'test']\n",
    "\n",
    "  for subdir in subdirs:\n",
    "    # create label subdirectories\n",
    "    for label in CATEGORIES:\n",
    "      new = path.join(dataset_path, subdir, label)\n",
    "      makedirs(new, exist_ok=True)\n",
    "    \n",
    "  # seed random number generator\n",
    "  seed(1)\n",
    "  \n",
    "  # copy training dataset images into subdirectories\n",
    "  for dir in CATEGORIES:\n",
    "    head = path.join(dataset_path, dir)\n",
    "    for file in listdir(head):\n",
    "      src = path.join(head, file)\n",
    "      dst = subdirs[0]\n",
    "      if random() < RATIO:\n",
    "        dst = subdirs[1]\n",
    "      check = file.rsplit(\"_\", 1)[1]\n",
    "      if check.startswith(dir):\n",
    "        dst = path.join(dataset_path, dst, dir, file)\n",
    "        copyfile(src, dst)\n",
    "\n",
    "  print('Directories are successfully created! Ready for modelling.')\n",
    "\n",
    "create_dir('large_dataset/dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96caa63c-6e58-4606-81c2-bb5b968ba244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17589 images belonging to 2 classes.\n",
      "Found 4479 images belonging to 2 classes.\n",
      "Initiated data generators.\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "path = 'large_dataset/dataset'\n",
    "# create data generators\n",
    "train_datagen = ImageDataGenerator(rotation_range=15,\n",
    "                                rescale=1./255,\n",
    "                                shear_range=0.1,\n",
    "                                zoom_range=0.2,\n",
    "                                horizontal_flip=True,\n",
    "                                width_shift_range=0.1,\n",
    "                                height_shift_range=0.1)\n",
    "\n",
    "# keep test images unmodified\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# prepare iterators\n",
    "train_it = train_datagen.flow_from_directory(os.path.join(path, 'train'),\n",
    "    class_mode='binary', batch_size=64, target_size=(SIZE, SIZE))\n",
    "test_it = test_datagen.flow_from_directory(os.path.join(path, 'test'),\n",
    "    class_mode='binary', batch_size=64, target_size=(SIZE, SIZE))\n",
    "\n",
    "print('Initiated data generators.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64800a6f-a7cd-4efd-a11a-34ff949daa66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /home/misu/.local/lib/python3.8/site-packages (1.6.3)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /home/misu/.local/lib/python3.8/site-packages (from scipy) (1.19.5)\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\n",
      "Callbacks are setup.\n",
      "Loaded model.\n",
      "Epoch 1/20\n",
      "275/275 [==============================] - 2096s 8s/step - loss: 3.3483 - accuracy: 0.8421 - val_loss: 9.1480 - val_accuracy: 0.5492\n",
      "Epoch 2/20\n",
      "275/275 [==============================] - 1977s 7s/step - loss: 0.1961 - accuracy: 0.9459 - val_loss: 0.7596 - val_accuracy: 0.9647\n",
      "Epoch 3/20\n",
      "275/275 [==============================] - 1978s 7s/step - loss: 0.1570 - accuracy: 0.9667 - val_loss: 1.4759 - val_accuracy: 0.9665\n",
      "Epoch 4/20\n",
      "275/275 [==============================] - 1951s 7s/step - loss: 0.1347 - accuracy: 0.9771 - val_loss: 16.0605 - val_accuracy: 0.8274\n",
      "Epoch 5/20\n",
      "275/275 [==============================] - 1842s 7s/step - loss: 0.1190 - accuracy: 0.9807 - val_loss: 58.2570 - val_accuracy: 0.7258\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "233/275 [========================>.....] - ETA: 4:17 - loss: 0.0756 - accuracy: 0.9882"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib\n",
    "!pip install scipy\n",
    "!apt-get install python-scipy\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "def create_model():\n",
    "# define cnn model (threeblock, dropout 25/50, batchnormalization, dense=512, rmsprop)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    # opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def load_path(path):\n",
    "    model = load_model(path)\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def summarize_diagnostics(history):\n",
    "\t# plot loss\n",
    "\tpyplot.subplot(211)\n",
    "\tpyplot.title('Cross Entropy Loss')\n",
    "\tpyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "\t# plot accuracy\n",
    "\tpyplot.subplot(212)\n",
    "\tpyplot.title('Classification Accuracy')\n",
    "\tpyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
    "\tpyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
    "\t# save plot to file\n",
    "\tfilename = sys.argv[0].split('/')[-1]\n",
    "\tpyplot.show()\n",
    "\tpyplot.savefig(filename + '_plot.png')\n",
    "\tpyplot.close()\n",
    "    \n",
    "def callbacks():\n",
    "    from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "    earlystop = EarlyStopping(patience = 10)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor = 'val_accuracy',patience = 2,verbose = 1,factor = 0.5,min_lr = 0.00001)\n",
    "    callbacks = [earlystop,learning_rate_reduction]\n",
    "\n",
    "    print('Callbacks are setup.')\n",
    "    return callbacks\n",
    "\n",
    "def run_harness():\n",
    "    \n",
    "    # model = load_path('furspect_model2_1b.h5')\n",
    "    model = create_model()\n",
    "    cb = callbacks()\n",
    "    \n",
    "    print('Loaded model.')\n",
    "    \n",
    "    # fit model\n",
    "    history = model.fit(train_it, steps_per_epoch=len(train_it),\n",
    "        validation_data=test_it, validation_steps=len(test_it), epochs=EPOCHS, verbose=1, callbacks=[cb], shuffle=True)\n",
    "    \n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(test_it, steps=len(test_it), verbose=1)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    \n",
    "    summarize_diagnostics(history)\n",
    "\n",
    "run_harness()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6937a1bc-7e35-4346-b623-fd626a0c7c8a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
